{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO6XpHgel4EJwWVJ1Q0aEiq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Lc4_S0RexvxF","executionInfo":{"status":"ok","timestamp":1721724529784,"user_tz":-180,"elapsed":4,"user":{"displayName":"Watene Keguti","userId":"00003334493310483030"}},"outputId":"843ff938-6898-4f4b-bbc3-329c47d1ce4f","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Final values: -0.3333333333333332 0.3333333333333332\n"]}],"source":["import numpy as np\n","\n","def gradient_descent(f, grad_f, x0, y0, learning_rate, max_iter):\n","  \"\"\"\n","  Performs gradient descent to minimize a function f.\n","\n","  Args:\n","    f: The function to be minimized.\n","    grad_f: The gradient of the function f.\n","    x0: The initial x-value.\n","    y0: The initial y-value.\n","    learning_rate: The learning rate for gradient descent.\n","    max_iter: The maximum number of iterations.\n","\n","  Returns:\n","    The final (x, y) values after optimization.\n","  \"\"\"\n","\n","  x = x0\n","  y = y0\n","\n","  for _ in range(max_iter):\n","    gradient = grad_f(x, y)\n","    x -= learning_rate * gradient[0]\n","    y -= learning_rate * gradient[1]\n","\n","  return x, y\n","\n","def f(x, y):\n","  return x**2 + y**2 - x*y + x - y + 1\n","\n","def grad_f(x, y):\n","  return [2*x - y + 1, 2*y - x - 1]\n","\n","# Initial values and parameters\n","x0 = 0\n","y0 = 0\n","learning_rate = 0.1\n","max_iter = 100\n","\n","# Perform gradient descent\n","x_final, y_final = gradient_descent(f, grad_f, x0, y0, learning_rate, max_iter)\n","\n","print(\"Final values:\", x_final, y_final)\n"]}]}